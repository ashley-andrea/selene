# ── LLM Provider ────────────────────────────────────────────────────────────
# Options: claude | crusoe | groq | ollama
LLM_PROVIDER=crusoe

# Crusoe Managed Inference (dev/staging — OpenAI-compatible, api.crusoe.ai)
CRUSOE_API_KEY=your_crusoe_api_key
CRUSOE_MODEL=meta-llama/Llama-3.3-70B-Instruct

# Groq (free-tier dev)
# GROQ_API_KEY=key
# GROQ_MODEL=llama-3.1-70b-versatile

# Claude (production)
# ANTHROPIC_API_KEY=key
# CLAUDE_MODEL=claude-sonnet-4-6

# Ollama (local)
# OLLAMA_MODEL=llama3.1

# ── ML Model APIs (Red Hat OpenShift) ──────────────────────────────────────
# Real deployed models on OpenShift
CLUSTER_API_URL=https://cluster-model-api-pietrosaveri-dev.apps.rm2.thpm.p1.openshiftapps.com/api/v1/cluster/predict
SIMULATOR_API_URL=https://simulator-model-api-pietrosaveri-dev.apps.rm2.thpm.p1.openshiftapps.com/api/v1/simulator/simulate

# ── Local dev fallback (mock server — deprecated) used only for when the ML models were not ready yet
# CLUSTER_API_URL=http://localhost:8001/cluster/predict
# SIMULATOR_API_URL=http://localhost:8001/simulator/simulate

# ── LangSmith Tracing (free tier) ──────────────────────────────────────────
LANGCHAIN_TRACING_V2=false
# LANGCHAIN_API_KEY=key
# LANGCHAIN_PROJECT=name
